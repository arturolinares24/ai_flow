{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "tDpL4nb7Nzg1"
      },
      "source": [
        "# Scripts del proyecto MLOps\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qADn_iJvNzhG"
      },
      "source": [
        "Script 1: Preparacion de datos para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from typing import Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wget\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def download_data(data_path: str) -> None:\n",
        "    \"\"\"Descarga los datos necesarios para el procesamiento desde una URL y los guarda en una carpeta local.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Ruta de la carpeta donde se guardarán los datos descargados.\n",
        "    \"\"\"\n",
        "    train_link = 'https://github.com/kubeflow/examples/blob/master/digit-recognition-kaggle-competition/data/train.csv.zip?raw=true'\n",
        "    wget.download(train_link, os.path.join(data_path, 'data_csv.zip'))\n",
        "\n",
        "    with zipfile.ZipFile(os.path.join(data_path, 'data_csv.zip'), \"r\") as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "\n",
        "\n",
        "def read_file_csv(filename: str, data_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Lee un archivo CSV y devuelve un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Nombre del archivo CSV.\n",
        "        data_path (str): Ruta de la carpeta donde se encuentra el archivo CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame creado a partir del archivo CSV.\n",
        "    \"\"\"\n",
        "    train_data_path = os.path.join(data_path, filename)\n",
        "    df = pd.read_csv(train_data_path)\n",
        "    print(filename, 'cargado correctamente')\n",
        "    return df\n",
        "\n",
        "\n",
        "def data_process(dataframe: pd.DataFrame) -> None:\n",
        "    \"\"\"Procesa los datos, dividiéndolos en conjuntos de entrenamiento, prueba y validación, y los guarda en archivos NPZ.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): DataFrame que contiene los datos a procesar.\n",
        "    \"\"\"\n",
        "    ntrain = dataframe.shape[0]\n",
        "\n",
        "    all_data_X = dataframe.drop('label', axis=1)\n",
        "    all_data_y = dataframe.label\n",
        "\n",
        "    all_data_X = all_data_X.values.reshape(-1, 28, 28, 1)\n",
        "    all_data_X = all_data_X / 255.0\n",
        "\n",
        "    X = all_data_X[:ntrain].copy()\n",
        "    y = all_data_y[:ntrain].copy()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n",
        "\n",
        "    np.savez(os.path.join('../data/processed/', 'train_data.npz'), X=X_train, y=y_train)\n",
        "    print('Data de entrenamiento exportada correctamente en la carpeta processed')\n",
        "\n",
        "    np.savez(os.path.join('../data/processed/', 'test_data.npz'), X=X_test, y=y_test)\n",
        "    print('Data de prueba exportada correctamente en la carpeta processed')\n",
        "\n",
        "    np.savez(os.path.join('../data/processed/', 'val_data.npz'), X=X_val, y=y_val)\n",
        "    print('Data de validacion exportada correctamente en la carpeta processed')\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Función principal para la preparación de datos.\"\"\"\n",
        "    data_path = \"../data/raw\"\n",
        "    download_data(data_path)\n",
        "    df1 = read_file_csv('train.csv', data_path)\n",
        "    data_process(df1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "uTexsXPqtPW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 2: Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "SgM_bOtUuO_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def load_data(filename: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Carga los datos de entrenamiento desde un archivo NPZ.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Nombre del archivo NPZ que contiene los datos.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: Tupla con los datos de entrada (X) y las etiquetas (y).\n",
        "    \"\"\"\n",
        "    data_path = os.path.join('../data/processed', filename)\n",
        "    train_data = np.load(data_path)\n",
        "    X_train, y_train = train_data['X'], train_data['y']\n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "def build_model(hidden_dim1: int, hidden_dim2: int, dropout_rate: float = 0.5) -> models.Sequential:\n",
        "    \"\"\"Construye y retorna un modelo secuencial de Keras.\n",
        "\n",
        "    Args:\n",
        "        hidden_dim1 (int): Número de filtros para la primera capa convolucional.\n",
        "        hidden_dim2 (int): Número de filtros para la segunda capa convolucional.\n",
        "        dropout_rate (float, optional): Tasa de dropout para las capas de dropout. Por defecto es 0.5.\n",
        "\n",
        "    Returns:\n",
        "        models.Sequential: Modelo secuencial construido.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(filters=hidden_dim1, kernel_size=(5, 5), padding='same', activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Conv2D(filters=hidden_dim2, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Conv2D(filters=hidden_dim2, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.build(input_shape=(None, 28, 28, 1))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model: models.Sequential, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                learning_rate: float, epochs: int, batch_size: int) -> tf.keras.callbacks.History:\n",
        "    \"\"\"Configura y entrena el modelo.\n",
        "\n",
        "    Args:\n",
        "        model (models.Sequential): Modelo de Keras a entrenar.\n",
        "        X_train (np.ndarray): Datos de entrada de entrenamiento.\n",
        "        y_train (np.ndarray): Etiquetas de entrenamiento.\n",
        "        learning_rate (float): Tasa de aprendizaje para el optimizador.\n",
        "        epochs (int): Número de épocas de entrenamiento.\n",
        "        batch_size (int): Tamaño del lote para el entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.callbacks.History: Historia del entrenamiento del modelo.\n",
        "    \"\"\"\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=[metrics.SparseCategoricalAccuracy(name='accuracy')])\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "    # Guardar el modelo\n",
        "    model_save_path = 'models/best_model.h5'\n",
        "    if not os.path.isdir('models'):\n",
        "        os.makedirs('models')\n",
        "    model.save(model_save_path)\n",
        "    print(f'Modelo guardado en {model_save_path}')\n",
        "\n",
        "    print('El entrenamiento del modelo ha finalizado con éxito.')\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Función principal para el entrenamiento del modelo.\"\"\"\n",
        "    # Hiperparámetros\n",
        "    LR = 1e-3\n",
        "    EPOCHS = 2\n",
        "    BATCH_SIZE = 64\n",
        "    CONV_DIM1 = 56\n",
        "    CONV_DIM2 = 100\n",
        "\n",
        "    X_train, y_train = load_data('train_data.npz')\n",
        "    model = build_model(CONV_DIM1, CONV_DIM2)\n",
        "    history = train_model(model, X_train, y_train, LR, EPOCHS, BATCH_SIZE)\n",
        "\n",
        "    print('El entrenamiento del modelo ha finalizado con éxito.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "L7bFQbWTuSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 3: Evaluacion del modelo"
      ],
      "metadata": {
        "id": "5ArHkj8JuoWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def load_data(filename: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Carga los datos de entrenamiento desde un archivo NPZ.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Nombre del archivo NPZ que contiene los datos.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: Tupla con los datos de entrada (X) y las etiquetas (y).\n",
        "    \"\"\"\n",
        "    data_path = os.path.join(\"../data/processed\", filename)\n",
        "    train_data = np.load(data_path)\n",
        "    X_train, y_train = train_data[\"X\"], train_data[\"y\"]\n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "def eval_model(X_test: np.ndarray, y_test: np.ndarray) -> None:\n",
        "    \"\"\"Evalúa el modelo cargado con los datos de prueba y muestra la matriz de confusión.\n",
        "\n",
        "    Args:\n",
        "        X_test (np.ndarray): Datos de entrada de prueba.\n",
        "        y_test (np.ndarray): Etiquetas de prueba.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Ruta al modelo guardado\n",
        "    model_path = \"models/best_model.h5\"\n",
        "\n",
        "    # Cargar el modelo\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    print(\"Modelo cargado exitosamente\")\n",
        "\n",
        "    # Evaluación del modelo\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test_loss: {test_loss}, Test_accuracy: {test_acc}\")\n",
        "\n",
        "    # Predicciones del modelo\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "    # Matriz de confusión\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Visualización de la matriz de confusión\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    sns.heatmap(cm, fmt=\"g\", cbar=False, annot=True, cmap=\"Blues\")\n",
        "    plt.title(\"Matriz de confusión\")\n",
        "    plt.ylabel(\"Etiqueta real\")\n",
        "    plt.xlabel(\"Etiqueta predicha\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Función principal para la evaluación del modelo.\"\"\"\n",
        "    X_test, y_test = load_data(\"test_data.npz\")\n",
        "\n",
        "    eval_model(X_test, y_test)\n",
        "    print(\"Finalizó la evaluación del modelo\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zCWZPXAfurFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 4: Scoring del modelo"
      ],
      "metadata": {
        "id": "ilvILbkyu4xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "def load_data(filename: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Carga los datos de entrenamiento desde un archivo NPZ.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Nombre del archivo NPZ.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: Datos de entrada y etiquetas cargados.\n",
        "    \"\"\"\n",
        "    data_path = os.path.join('../data/processed', filename)\n",
        "    train_data = np.load(data_path)\n",
        "    X_train, y_train = train_data['X'], train_data['y']\n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "def score_model(X_val: np.ndarray, y_val: np.ndarray) -> None:\n",
        "    \"\"\"Evalúa el modelo cargado y guarda los resultados en un archivo CSV.\n",
        "\n",
        "    Args:\n",
        "        X_val (np.ndarray): Datos de entrada de validación.\n",
        "        y_val (np.ndarray): Etiquetas de validación.\n",
        "    \"\"\"\n",
        "    # Ruta al modelo guardado\n",
        "    model_path = 'models/best_model.h5'\n",
        "\n",
        "    # Cargar el modelo\n",
        "    model = load_model(model_path)\n",
        "    print(\"Modelo cargado exitosamente\")\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_val), axis=-1)\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Real': y_val,\n",
        "        'Predicted': y_pred\n",
        "    })\n",
        "    results_df.to_csv(os.path.join('../data/scores/score.csv'),index=False)\n",
        "    print('Exportación correctamente en la carpeta scores')\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Función principal para la evaluación del modelo.\"\"\"\n",
        "    X_test, y_test = load_data('val_data.npz')\n",
        "    score_model(X_test, y_test)\n",
        "    print('Finalizó la validación del Modelo')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ADoBYxiVu-JI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "kubeflow_notebook": {
      "autosnapshot": true,
      "experiment": {
        "id": "new",
        "name": "digit-recognizer-kale"
      },
      "experiment_name": "digit-recognizer-kale",
      "katib_metadata": {
        "algorithm": {
          "algorithmName": "grid"
        },
        "maxFailedTrialCount": 3,
        "maxTrialCount": 12,
        "objective": {
          "objectiveMetricName": "",
          "type": "minimize"
        },
        "parallelTrialCount": 3,
        "parameters": []
      },
      "katib_run": false,
      "pipeline_description": "Performs Preprocessing, training and prediction of digits",
      "pipeline_name": "digit-recognizer-kale",
      "snapshot_volumes": true,
      "steps_defaults": [
        "label:access-ml-pipeline:true",
        "label:access-rok:true"
      ],
      "volume_access_mode": "rwm",
      "volumes": [
        {
          "annotations": [],
          "mount_point": "/home/jovyan",
          "name": "arikkto-workspace-7xzjm",
          "size": 5,
          "size_type": "Gi",
          "snapshot": false,
          "type": "clone"
        }
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}